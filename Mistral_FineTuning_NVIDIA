{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tuning Mistral 7B using QLoRA \n\nMistral 7B is a recent open-source language model developed by MistralAI that consistently delivers state-of-the-art results across a variety of natural language understanding and generation benchmarks. While this model serves as a strong baseline for multiple downstream tasks, it can lack in domain-specific knowledge or proprietary or otherwise sensitive information. Fine-tuning is often used as a means to update a model for a specific task or tasks to better respond to domain-specific prompts. This notebook walks through downloading the Mistral 7B model from Hugging Face, preparing a custom dataset on coding-related tasks and instructions, and using Quantized Low Rank Adaptation (QLoRA) to fine-tune the base model against the dataset. While we focus on a coding-specific task in this example, this methodology can be applied seamlessly to other tasks as well. \n\nThis workflow is inspired by the posts and repositories [here](https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-mistral-7b-instruct-model-0f39647b20fe) and [here](https://github.com/brevdev/notebooks/blob/main/mistral-finetune.ipynb)\n\n### 0. What is LoRA? QLoRA?\nWith regards to Large Language Models (LLMs), fine-tuning is the customization of pretrained models, like Mistral-7B, towards new or more domain-specific instructions and data. This process updates the model weights through retraining either all the parameters of the model (in full fine-tuning), or a certain subset of them (in parameter-efficient fine-tuning, or PEFT). Full fine-tuning may produce better results, but in many cases PEFT is preferred due to it being lesser time-consuming and resource-intensive. \n\nLow-Rank Adaptation, or LoRA, is a method of PEFT that uses smaller weight matrices in the retraining as approximation instead of updating the full weight matrix. This rank decomposition optimization technique enables greater memory efficiency and can reduce the size of GPU required in order to perform the fine-tuning successfully. \n\nQLoRA is a further optimization that reduces the precision of the model weights as well in order to provide even greater advances in memory and space efficiency. The most common quantization used for this LoRA finetuning workflow is 4-bit quantization, which provides a decent balance between model performance, and fine-tuning feasibility. In theory, incorporating these optimizations means this workflow can even work on an NVIDIA RTX 3090!\n\nAlright, enough chit-chat. Let's dive in!\n\nFirst, let's select the level of quantization we would like to use for this fine-tuning project. Choose from ``None``, ``8bit``, or ``4bit`` quantization levels. Keep in mind that the ``None`` option defaults to full 16 bit precision, which mean this workflow will perform **standard LoRA** fine-tuning, while the other two options apply quantization for QLoRA fine-tuning.","metadata":{"_uuid":"f08472e1-d9de-4e23-a2ea-065f2c4444e6","_cell_guid":"e009b01e-f187-47af-a377-3a5362b49160","id":"XIyP_0r6zuVc","trusted":true}},{"cell_type":"code","source":"# python packages to install, one per line\ntransformers==4.35.0\npeft==0.5.0\nbitsandbytes==0.42.0\naccelerate==0.22.0\ntrl==0.7.2\npydantic-settings==2.0.3\nscipy==1.11.4\njupyterlab","metadata":{"_uuid":"e0cae88a-4562-49b3-bcf5-742917efd967","_cell_guid":"4c752b58-cb2a-4ad3-b9a8-8e4765f946e5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DEFINE QUANTIZATION HERE. Choose from (\"none\" | \"8bit\" | \"4bit\")\nQUANTIZATION = \"4bit\"","metadata":{"_uuid":"32e8d135-7917-4beb-8418-7d0abeeafb6e","_cell_guid":"3753383e-8249-46ff-88c0-7b42fdcaadfe","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's set our imports.","metadata":{"_uuid":"2a46083b-b638-4ae5-8534-461a737e701a","_cell_guid":"f48efb14-37fb-4324-9c62-1c0587d61f26","trusted":true}},{"cell_type":"code","source":"import os\nimport torch\nimport transformers\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel","metadata":{"_uuid":"3b3cdeff-83bd-4a8a-9520-586fcb54f172","_cell_guid":"14429638-188a-4079-99a5-3b002215ba71","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Load in the Dataset","metadata":{"_uuid":"1f5a0dea-c456-4fd3-aef9-4461d6289444","_cell_guid":"220efbe6-aec5-4c1f-bc0e-f81bd2d9842d","id":"QcE4NTeFyRgd","trusted":true}},{"cell_type":"markdown","source":"While the pretrained Mistral model has some degree of code understanding and generation in addition to English natural language processing tasks, it still falls short in certain cases, which we will explore later in this notebook. For this workflow, we will aim to fine-tune the Mistral 7B model to generate high quality responses to code generation tasks. \n\nTo accomplish this, we will be using [this dataset](https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style) from HuggingFace that consists of 122k code instructions that follow the alpaca style of instructions, as well as the ground truth outputs we expect our model to produce. Let's go ahead and load in the dataset, and split the entries into train, test, and validation sets.","metadata":{"_uuid":"121ee905-851d-43a3-9f04-0375d57e0df1","_cell_guid":"49c52be2-7623-4ba3-8934-299a593151a5","id":"FCc64bfnmd3j","trusted":true}},{"cell_type":"code","source":"dataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split='train')\ndataset = dataset.train_test_split(test_size=0.2)\nval_test_dataset = dataset['test'].train_test_split(test_size=0.5)\n\ntrain_dataset = dataset[\"train\"]\neval_dataset = val_test_dataset[\"train\"]\ntest_dataset = val_test_dataset[\"test\"]","metadata":{"_uuid":"8d1ee268-0b58-4a32-b353-cd4b3f6b3c8f","_cell_guid":"066f1cc1-3111-4139-ad2d-9ec5796d2f42","collapsed":false,"id":"s6f4z8EYmcJ6","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check that our data splits are correct.","metadata":{"_uuid":"3f57bb7c-3dd2-4924-b4a4-d0bf18023cad","_cell_guid":"96f483e7-c965-4c5a-85b2-70e54ed04d6f","trusted":true}},{"cell_type":"code","source":"print(train_dataset)\nprint(eval_dataset)\nprint(test_dataset)","metadata":{"_uuid":"7ed1033f-414a-4e78-b368-a8cbb44a83a0","_cell_guid":"ca054155-0ac7-432d-b2f9-a015015f4e7f","collapsed":false,"id":"EmZbX-ltyRge","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Load In the Base Model","metadata":{"_uuid":"96ee1901-a20b-4398-9d8a-ebcd4fef1aff","_cell_guid":"ba5eef3a-afbd-40e6-b3b4-c666e47988d8","id":"shz8Xdv-yRgf","trusted":true}},{"cell_type":"markdown","source":"Now, let's now load in the Mistral Model from Huggingface - `mistralai/Mistral-7B-v0.1`. We will aim to use 4-bit quantization, which is a method that significantly reduces the overall memory footprint of the fine-tuning process by reducing precision of the model parameters while preserving performance. This makes it easier to run this fine-tuning workflow on smaller GPU systems, not just A100s!","metadata":{"_uuid":"7cf2da60-7aab-4407-a9a5-dd5c697a2ebb","_cell_guid":"b2704396-ce7a-49be-a37d-aa2d7596f8a7","id":"MJ-5idQwzvg-","trusted":true}},{"cell_type":"code","source":"# Pre-define quantization configs\n\n################## 4bit ##################\nbb_config_4b = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n##########################################\n\n################## 8bit ##################\nbb_config_8b = BitsAndBytesConfig(\n    load_in_8bit=True,\n)\n##########################################\n\ndef quantization_config(quantization):\n    if quantization == \"8bit\":\n        return bb_config_8b\n    else:\n        return bb_config_4b","metadata":{"_uuid":"1f7951eb-c961-453f-848a-5ca69763b4f7","_cell_guid":"e1e087bc-f1bc-4552-9059-79b2356e3bc8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_id = \"mistralai/Mistral-7B-v0.1\"\nhf_api_token = os.environ['HUGGING_FACE_HUB_TOKEN']\n\nif QUANTIZATION == \"none\":\n    model = AutoModelForCausalLM.from_pretrained(model_id, token=hf_api_token).to(\"cuda\")\nelse: \n    model = AutoModelForCausalLM.from_pretrained(model_id, token=hf_api_token, quantization_config=quantization_config(QUANTIZATION))","metadata":{"_uuid":"05471cba-dd2a-4357-91e4-4b9ee745cc38","_cell_guid":"bb13026a-01ea-4ff7-9241-2f1cc5ffe90e","collapsed":false,"id":"E0Nl5mWL0k2T","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Evaluate Base Model Performance","metadata":{"_uuid":"3119ffe5-123b-449d-9095-e347de0b3114","_cell_guid":"ebc98ad3-8725-4674-baf5-9e9cb860b31e","id":"7fi9wEZYyRgh","trusted":true}},{"cell_type":"markdown","source":"Before fine-tuning the model, let's first evaluate how well the model does on sample tasks that we intend to fine-tune on, such as generating functions in code, coding syntax and semantics, and general understanding of multiple coding languages. Here, we'll ask it a fairly standard coding question: Write a function to output the prime factorization of 2023 in python, C, and C++.","metadata":{"_uuid":"b2aee6fe-1986-4567-add4-cc38e832aa2b","_cell_guid":"59ad6b61-3400-4db0-a270-ce6fde1b9c17","id":"Vxbl4ACsyRgi","trusted":true}},{"cell_type":"code","source":"base_prompt = \"\"\"Write a function to output the prime factorization of 2023 in python, C, and C++\"\"\"","metadata":{"_uuid":"2b0293b0-1901-4919-b812-9b146d7e4c01","_cell_guid":"f12d2a98-86ee-4c5e-a50b-977b6b4c0845","collapsed":false,"id":"gOxnx-cAyRgi","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's call the model and see what it outputs.","metadata":{"_uuid":"17d7c4f1-4620-4b70-b322-025a594c8c8d","_cell_guid":"d998ae2c-4498-4eee-ab91-9c2e7b55fca7","trusted":true}},{"cell_type":"code","source":"base_tokenizer = AutoTokenizer.from_pretrained(\n    model_id,\n    add_bos_token=True,\n)\n\nmodel_input = base_tokenizer(base_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nmodel.eval()\nwith torch.no_grad():\n    print(base_tokenizer.decode(model.generate(**model_input, max_new_tokens=256)[0], skip_special_tokens=True))","metadata":{"_uuid":"2fd32599-bad3-4fa4-a93d-66fab2bcbe04","_cell_guid":"4a3a85dd-7cda-485a-837d-42e7773d3fa2","collapsed":false,"id":"NidIuFXMyRgi","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see it doesn't do very well out of the box...\n\n1. The out-of-the-box model seems to think the prime factorization of 2023 is 13 x 157. This amounts to 2041! The actual answer is 7 x 17 x 17. \n\n2. At first glance the python function it outputs is incorrect as well; if we actually run the code, it gives the answer as ``[7, 17, 119, 289, 2023]``. 119, 289, and of course 2023 are not prime factors! \n\nWhile the syntax is generally comprehensible, we can see that there are still issues in the output that could be improved on. Let's attempt to improve the quality of the model's outputs using fine-tuning.","metadata":{"_uuid":"d6e63b2a-c37f-4483-8358-78f355c33e92","_cell_guid":"21120d24-1272-4f5d-901e-11c154d65648","id":"dCAWeCzZyRgi","trusted":true}},{"cell_type":"markdown","source":"### 4. Format the Data for Fine-Tuning\n\nLet's first set up the tokenizer before formatting the dataset. Left-padding is recommended here as it can [reduce memory costs](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa).","metadata":{"_uuid":"1fec34e1-dd9d-425f-aeca-54280f1f13a1","_cell_guid":"86321554-683a-49db-b5a0-4b78601754e8","id":"UjNdXolqyRgf","trusted":true}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    model_id,\n    model_max_length=512,\n    padding_side=\"left\",\n    add_eos_token=True)\n\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize(prompt):\n    tokenized = tokenizer(\n        prompt,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n    )\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized","metadata":{"_uuid":"6db48f40-fc6b-4670-9953-0ea7de5779d8","_cell_guid":"a57e32b7-9069-4bbb-a6e1-06e93c1d0a79","collapsed":false,"id":"haSUDD9HyRgf","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can then reformat the dataset to fit the instruction prompt for fine-tuning. We will enclose the instruction and any inputs given to the model in a ``[INST]`` tag, and attach the correct output afterwards. \n\nThen, we tokenize each entry of our dataset using the tokenizer we set up above.","metadata":{"_uuid":"836e5a65-c3c4-4283-aa9e-1eb6f2aba0d4","_cell_guid":"ad698119-8582-4540-8c08-ed142c8b91f9","id":"tJtsbrr6yRgg","trusted":true}},{"cell_type":"code","source":"def process_prompt(data):\n    new_prompt = f\"\"\"<s>[INST] {data[\"instruction\"]} here are the inputs {data[\"input\"]} [/INST] \\\\n {data[\"output\"]} </s>\"\"\"\n    return tokenize(new_prompt)\n\ntokenized_train_ds = train_dataset.map(process_prompt)\ntokenized_val_ds = eval_dataset.map(process_prompt)","metadata":{"_uuid":"e73c0811-213c-49c7-9f2f-f94dde46f92c","_cell_guid":"5fdf15af-54e9-4b63-a5e2-8691b5605834","collapsed":false,"id":"6z9rvnoDyRgg","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Set up for QLoRA Fine-Tuning","metadata":{"_uuid":"cf23cef3-5232-48b9-99cf-2c108e9acdd7","_cell_guid":"60b83cc2-0913-4a54-80ab-50b5a4ca7bd2","id":"AapDoyfAyRgi","trusted":true}},{"cell_type":"markdown","source":"Now, we are ready to set up our fine-tuning workflow. Let's prepare the model for parameter efficient fine-tuning. We'll also implement a neat function to let us know exactly how many of the model weights will be retrained and how many will be frozen, just to get a good idea for how PEFT is working under the hood.","metadata":{"_uuid":"4eb04d6a-fe26-4987-91ff-bf4d1f338300","_cell_guid":"e3f3435d-f092-4d14-8050-e68532a0b9ef","id":"Mp2gMi1ZzGET","trusted":true}},{"cell_type":"code","source":"model.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n\ndef print_param_info(model):\n    \"\"\"\n    Outputs trainable parameter information.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"_uuid":"11f838d6-424b-4fe5-8df9-2656612432d7","_cell_guid":"5c761bac-0ec4-4b95-99f1-8d211215bee8","collapsed":false,"id":"gkIcwsSU01EB","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we can print out the architecture of the model. QLoRA will be applied to all the linear layers of this model.","metadata":{"_uuid":"4ed39dfa-fd06-4132-b8af-6b94c2191bcb","_cell_guid":"6e5195cb-a53c-4643-a565-6c0f1eeda0dc","id":"cUYEpEK-yRgj","trusted":true}},{"cell_type":"code","source":"print(model)","metadata":{"_uuid":"41aedef5-8311-4601-b7f2-99da219b4804","_cell_guid":"3396f4f5-78bb-47a6-bf5d-4f45fafd0efa","collapsed":false,"id":"XshGNsbxyRgj","scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see those layers are: \n* `q_proj`\n* `k_proj`\n* `v_proj`\n* `o_proj`\n* `gate_proj`\n* `up_proj`\n* `down_proj`\n* `lm_head`\n\nLet's make a note of these and pass them into the LoRA config. You are also able to specify a few other fields in the config. For example: \n\n* `r`: This field refers to the rank of the lower-rank matrices you want to use in the adaptation layers of the model, which controls the number of parameters set to be retrained. The higher this number, the more expressiveness you will capture; however, there is added computational cost. \n\n* `alpha`: This field refers to the scaling factor for the weights. The weights are scaled by a factor of `alpha/r`, and so the higher this number means more weights are assigned to the LoRA activations.\n\nThe authors of the original QLoRA paper used the following values: `r=64` and `lora_alpha=16`. While these may be able to generalize well, let's set the defaults here to `r=8` and `lora_alpha=16`. This way, we allocate a greater amount of weights as retrainable on the new fine-tuned data while also minimizing computational complexity. You are free to adjust and tune these parameters as you wish. \n\nLet's use the ``print_param_info`` defined above to see what the trainable and frozen parameters look like.","metadata":{"_uuid":"5f3c11e3-a781-45ca-a4a9-ce86e1d32036","_cell_guid":"68505ad4-7559-46e6-86d7-4adf61958bb1","id":"I6mTLuQJyRgj","trusted":true}},{"cell_type":"code","source":"config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, config)\nprint_param_info(model)","metadata":{"_uuid":"85f94ce7-48f2-41c1-be88-43bb64b88532","_cell_guid":"04ff3235-6135-44c9-916f-a79401aca036","collapsed":false,"id":"Ybeyl20n3dYH","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And reprinting the model architecture shows us the updated model with proper quantization and LoRA layers wrapping the original linear layers.","metadata":{"_uuid":"6ee56731-4df3-43d1-b523-f381c819a8f7","_cell_guid":"ae5db263-a837-4e46-afe0-0cfac6ce1b25","id":"X_FHi_VLyRgn","trusted":true}},{"cell_type":"code","source":"print(model)","metadata":{"_uuid":"a032ff2e-6a7c-4949-a774-ff4db3138846","_cell_guid":"5639ab30-b85f-4629-af1e-7edea0d9593a","collapsed":false,"id":"IaYMWak4yRgn","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Run QLoRA Fine-Tuning","metadata":{"_uuid":"260b345c-273e-4518-b0a0-3647ea8d72fb","_cell_guid":"abbe6cab-2873-47d8-a07b-3454444be243","id":"_0MOtwf3zdZp","trusted":true}},{"cell_type":"markdown","source":"Now with the dataset processed and tokenized, and with the model prepared, we are ready to begin running the fine-tuning. This following cell will configure the trainer object with various default parameters. \n\nOn an 1x A100-80GB system, this cell can take several hours to complete as-written. Depending on your hardware and patience, you may need to adjust certain parameters to achieve reasonable training times. Notably, we set the `max_steps` to 1000 and the checkpoint and evaluation to every 50 steps; you may reduce the number of steps and/or make less frequent checkpoints if you would like to reduce the training time. \n\nFor your convenience, a progress bar is generated, as well as checkpointing for the training and validation errors. If the validation error begins increasing, you may be running into issues with model overfitting. At this point, you may interrupt the kernel to stop training, and pass the appropriate ``checkpoint-xx`` to Step 7.","metadata":{"_uuid":"f18bd811-11c7-4d9d-ae86-94a7a11b4750","_cell_guid":"587e1c43-89c4-455c-a70c-753f09742a85","id":"fEe0uWYSyRgo","trusted":true}},{"cell_type":"code","source":"# Parallelization is possible if system is multi-GPU\nif torch.cuda.device_count() > 1: \n    model.is_parallelizable = True\n    model.model_parallel = True\n\ntokenizer.pad_token = tokenizer.eos_token\n\n# Training configs\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=tokenized_train_ds,\n    eval_dataset=tokenized_val_ds,\n    args=transformers.TrainingArguments(\n        output_dir=\"./mistral-code-instruct\",\n        warmup_steps=5,\n        per_device_train_batch_size=2,\n        gradient_checkpointing=True,\n        gradient_accumulation_steps=4,\n        max_steps=500,\n        learning_rate=2.5e-5,\n        logging_steps=50,\n        bf16=True if (QUANTIZATION != \"8bit\") else False,\n        fp16=True if (QUANTIZATION == \"8bit\") else False,\n        optim=\"paged_adamw_8bit\",\n        logging_dir=\"./logs\",\n        save_strategy=\"steps\",\n        save_steps=50,\n        evaluation_strategy=\"steps\", \n        eval_steps=50,\n        report_to=\"none\",\n        do_eval=True,\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\n# Silencing warnings. If using for inference, consider re-enabling.\nmodel.config.use_cache = False \n\n# Train! \ntrainer.train()","metadata":{"_uuid":"2340f6fa-e30c-4fcb-81b9-750d1714cf65","_cell_guid":"09786685-6dcf-471f-bc96-6501bbf9e598","collapsed":false,"id":"jq0nX33BmfaC","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. Evaluate the Fine-Tuned Model\n\nGood news, the model is now fine-tuned to your dataset! \n\nIf you find you are running low on VRAM, you may consider restarting the kernel at this point. The PEFT library functionality saves only the QLoRA adapters in the checkpoints by default, and so the original weights need to be reloaded. Restarting the kernel may prevent any out-of-memory headaches when loading the base model again on top of this customized model we just fine-tuned. \n\nIn case you restarted the kernel, let's redefine everything again.","metadata":{"_uuid":"582c2340-2b95-48a3-b134-e352a124caa0","_cell_guid":"7bbac0cf-7205-4f05-851d-c567c25f1d79","id":"0D57XqcsyRgo","trusted":true}},{"cell_type":"code","source":"import os\nimport torch\nimport transformers\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n\n# Pre-define quantization configs\n\n################## 4bit ##################\nbb_config_4b_eval = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n##########################################\n\n################## 8bit ##################\nbb_config_8b_eval = BitsAndBytesConfig(\n    load_in_8bit=True,\n)\n##########################################\n\ndef quantization_config_eval(quantization):\n    if quantization == \"8bit\":\n        return bb_config_8b_eval\n    else:\n        return bb_config_4b_eval","metadata":{"_uuid":"b2e4ff55-d6bc-42d6-818d-71ed6789554c","_cell_guid":"0f044f5a-6670-47c2-8ddf-808130150993","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_id = \"mistralai/Mistral-7B-v0.1\"\nhf_api_token = os.environ['HUGGING_FACE_HUB_TOKEN']\n\nif QUANTIZATION == \"none\":\n    base_model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        token=hf_api_token, \n        device_map=\"auto\",\n        trust_remote_code=True,\n    ).to(\"cuda\")\nelse: \n    base_model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        token=hf_api_token, \n        quantization_config=quantization_config_eval(QUANTIZATION),\n        device_map=\"auto\",\n        trust_remote_code=True,\n    )\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_id,\n    model_max_length=512,\n    padding_side=\"left\",\n    add_eos_token=True)\n\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"_uuid":"6e0a4fc0-be19-4b77-9240-45613f8761e7","_cell_guid":"6d024800-d54a-43ab-9653-38004fe80e9a","collapsed":false,"id":"SKSnF016yRgp","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can merge our updated model weights from the QLoRA training with the original weights of the base Mistral model. Make sure you choose the best performing model checkpoint.","metadata":{"_uuid":"401e1ff1-102b-4668-b3cd-5a572fdeb617","_cell_guid":"7793c5c4-ab8c-4d29-b41a-bac60e08c9b8","id":"_BxOhAiqyRgp","trusted":true}},{"cell_type":"code","source":"ft_model = PeftModel.from_pretrained(base_model, \"mistral-code-instruct/checkpoint-500\")","metadata":{"_uuid":"32005c9c-e984-4f10-9923-66dfeead5021","_cell_guid":"8bf63072-d5ec-43ea-8909-79feb740292d","collapsed":false,"id":"GwsiqhWuyRgp","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are ready to use this merged model for inference! Let's go ahead and try a similar prime factorization programming question to what we had asked above, and see if our fine-tuned Mistral model achieves better quality responses.","metadata":{"_uuid":"1c213ddd-1945-4b05-ac9d-771c9d9bd1d3","_cell_guid":"77407264-5eff-4f38-a2ed-0f789e0d526f","id":"UUehsaVNyRgp","trusted":true}},{"cell_type":"code","source":"eval_prompt = f\"\"\"<s>\nFor a given integer n, print out all its prime factors one on each line. \nn = 30\n[INST]\n\"\"\"\n\ninput_ids = tokenizer(eval_prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\noutputs = ft_model.generate(input_ids=input_ids, max_new_tokens=256, do_sample=True, top_p=0.9,temperature=0.5)\n\nprint(f\"Prompt:\\n{eval_prompt}\\n\")\nprint(f\"\\nGenerated response:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(eval_prompt):]}\")\nprint('''\\nGround truth:\\ndef print_prime_factors(n): \n  for i in range(2, n + 1):\n    while n % i == 0:\n      print(i)\n      n //= i\nprint_prime_factors(n)''')","metadata":{"_uuid":"8d459cde-a14d-426b-8cd3-badf8a1ac086","_cell_guid":"6cb1ac53-40ce-4542-bdf1-be9a3af2407b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"30 can be factored into the following primes: 2, 3, 5. Because we fine tune on generating code snippets and not answering the question posed to the LLM, you may in some cases see 'hallucinatory' returned answers that may not align perfectly with the actual ouput of the generated code snippet, so be sure to examine the generated code snippet rather than solely relying on the returned outputted response or answer. Feel free to spin up a sandbox environment to evaluate any generated code. \n\nCheck out the ``Generated response`` output and compare it with the ``Ground truth`` code. Try out the ``Generated response`` yourself in a sandbox environment. Could you be underfitting? Overfitting? Or does the code work as intended? \n\nIf so, nice! Using QLoRA fine-tuning, we can now generate comprehensible and accurate code that accomplishes what the out-of-the-box baseline Mistral model was unable to achieve. Now, feel free to adjust the hyperparameters, bring in your own custom data, or customize this fine-tuning workflow to improve model performance for your particular use case.","metadata":{"_uuid":"8ed4eb80-6d24-4ad0-becd-d5d93a6d54b5","_cell_guid":"f901dc8d-7b12-46d0-a8b0-fefc0b2becdb","trusted":true}},{"cell_type":"markdown","source":"### 7. Merge and Save the Fine-tuned Model\n\nNow, we are ready to save the fine tuned model weights to the base model. Let's save this in under `models`, which you have already mounted to your host system for easy access.","metadata":{"_uuid":"d71a13b1-05c4-40cb-bd76-a227bc04564d","_cell_guid":"70c909da-8c9b-47c6-a47b-35407f1942a4","trusted":true}},{"cell_type":"code","source":"ft_model.save_pretrained(\"/project/models/finetuned\")","metadata":{"_uuid":"161dad93-65e9-4b46-a05a-a90e9bf40c0f","_cell_guid":"d900369e-d09d-43bd-af44-8f81570dce21","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"b96a5c1e-71a6-4158-94e6-2168ef8f1ce2","_cell_guid":"1cdeef64-472f-4dc2-9adc-ce99a573ea03","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}