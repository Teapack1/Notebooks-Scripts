{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in e:\\user\\majoron\\python\\project\\notebooks-scripts\\venv\\lib\\site-packages (1.13.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in e:\\user\\majoron\\python\\project\\notebooks-scripts\\venv\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: tqdm>4 in e:\\user\\majoron\\python\\project\\notebooks-scripts\\venv\\lib\\site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in e:\\user\\majoron\\python\\project\\notebooks-scripts\\venv\\lib\\site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in e:\\user\\majoron\\python\\project\\notebooks-scripts\\venv\\lib\\site-packages (from openai) (2.6.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in e:\\user\\majoron\\python\\project\\notebooks-scripts\\venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in e:\\user\\majoron\\python\\project\\notebooks-scripts\\venv\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: sniffio in e:\\user\\majoron\\python\\project\\notebooks-scripts\\venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in e:\\user\\majoron\\python\\project\\notebooks-scripts\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in e:\\user\\majoron\\python\\project\\notebooks-scripts\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\user\\majoron\\python\\project\\notebooks-scripts\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: certifi in e:\\user\\majoron\\python\\project\\notebooks-scripts\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in e:\\user\\majoron\\python\\project\\notebooks-scripts\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in e:\\user\\majoron\\python\\project\\notebooks-scripts\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in e:\\user\\majoron\\python\\project\\notebooks-scripts\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: colorama in e:\\user\\majoron\\python\\project\\notebooks-scripts\\venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A model that takes in a puzzle-like reasoning-heavy question in English, and responds with a well-reasoned, step-by-step thought out response in Spanish.\"\n",
    "temperature = .4\n",
    "number_of_examples = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating example 0\n",
      "[\"Let's start generating some prompt/response pairs for the model. \\n\\nprompt\\n-----------\\nWhat is the capital of France?\\n-----------\\n\\nresponse\\n-----------\\nLa capital de Francia es ParÃ­s.\\n-----------\"]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import random\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def generate_example(prompt, prev_examples, temperature=.5):\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    if len(prev_examples) > 0:\n",
    "        if len(prev_examples) > 10:\n",
    "            prev_examples = random.sample(prev_examples, 10)\n",
    "        for example in prev_examples:\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": example\n",
    "            })\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=1354,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Generate examples\n",
    "prev_examples = []\n",
    "for i in range(number_of_examples):\n",
    "    print(f'Generating example {i}')\n",
    "    example = generate_example(prompt, prev_examples, temperature)\n",
    "    prev_examples.append(example)\n",
    "\n",
    "print(prev_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system message is: `Given a reasoning-heavy question in English, you will solve the puzzle and provide a step-by-step reasoned response in Spanish.`. Feel free to re-run this cell if you want a better result.\n"
     ]
    }
   ],
   "source": [
    "def generate_system_message(prompt):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "          {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You will be given a high-level description of the model we are training, and from that, you will generate a simple system prompt for that model to use. Remember, you are not generating the system message for data generation -- you are generating the system message to use for inference. A good format to follow is `Given $INPUT_DATA, you will $WHAT_THE_MODEL_SHOULD_DO.`.\\n\\nMake it as concise as possible. Include nothing but the system prompt in your response.\\n\\nFor example, never write: `\\\"$SYSTEM_PROMPT_HERE\\\"`.\\n\\nIt should be like: `$SYSTEM_PROMPT_HERE`.\"\n",
    "          },\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt.strip(),\n",
    "          }\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "system_message = generate_system_message(prompt)\n",
    "\n",
    "print(f'The system message is: `{system_message}`. Feel free to re-run this cell if you want a better result.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\User\\majoron\\Python\\project\\notebooks-scripts\\venv\\lib\\site-packages\\pyarrow\\compute.py:126: RuntimeWarning: coroutine 'AsyncCompletions.create' was never awaited\n",
      "  name=func.name,\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 successfully-generated examples. Here are the first few:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the best strategy for solving a comple...</td>\n",
       "      <td>La mejor estrategia para resolver un rompecabe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  What is the best strategy for solving a comple...   \n",
       "\n",
       "                                            response  \n",
       "0  La mejor estrategia para resolver un rompecabe...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store prompts and responses\n",
    "prompts = []\n",
    "responses = []\n",
    "\n",
    "# Parse out prompts and responses from examples\n",
    "for example in prev_examples:\n",
    "  try:\n",
    "    split_example = example.split('-----------')\n",
    "    prompts.append(split_example[1].strip())\n",
    "    responses.append(split_example[3].strip())\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'prompt': prompts,\n",
    "    'response': responses\n",
    "})\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print('There are ' + str(len(df)) + ' successfully-generated examples. Here are the first few:')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets, with 90% in the train set\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "# Save the dataframes to .jsonl files\n",
    "train_df.to_json('train.jsonl', orient='records', lines=True)\n",
    "test_df.to_json('test.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the best strategy for solving a comple...</td>\n",
       "      <td>La mejor estrategia para resolver un rompecabe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  What is the best strategy for solving a comple...   \n",
       "\n",
       "                                            response  \n",
       "0  La mejor estrategia para resolver un rompecabe...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
